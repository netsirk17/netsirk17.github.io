---
layout:     post
title:      "Surviving the Titanic"
subtitle:   "Predicting Disaster Survival with Classification Models"
date:       2016-10-26 12:00:00
author:     "Kristen Su"
header-img: "img/project-5/titanic-sinking.jpg"
---

<div>
<h2 class="section-heading">Executive Summary</h2>

  <p><b> Dataset: <a href="https://www.kaggle.com/c/titanic/data"> Titanic Database</a> </b></p>
  <p> The goal of the project was to predict the chances of any one passenger surviving the Titanic sinking of 1912, using various classification models to come to the best predictions. The best model turned out to be a cross-validated logistic regression model  with 82.6% accuracy, 80.0% precision and a 12.9% false positive rate. This model was built with LogisticRegressionCV from scikitlearn's linear_model module. 
  </p>

  <p><b>Project Details</b>:
  <ol>
    <li> Acquire data from AWL PostgreSQL database </li>
    <li> Create classification models to predict likelihood of survival. I built the following models: </li>
      <ul>
        <li>Logistic Regression</li>
        <li>Cross-validated Logistic Regression</li>
        <li>GridSearch Logistic Regression</li>
        <li>kNN Model</li>
        <li>GridSearch kNN Model</li>
        <li>GridSearch SVM Model</li>
        <li>GridSearch Decision Trees Model</li>
      </ul>
    <li> Examine models through confusion matrices and ROC curves </li>
  </ol>

  <p> This project was inspired by a <a href="https://www.kaggle.com/c/titanic)">Kaggle competition</a><br>
  </p>

<h2 class="section-heading">Women and Children First?</h2>
<p><br>
</p>
<p>
</p>

<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/womenchildren.jpg"></a>
</div>

<p><br>
</p>
<p>
</p>

<p> Remember that line from James Cameron's 1997 classic film <i>Titanic</i>? A chivalrous sentiment but apparently not maritime law in 1912. However, looking over the data it appears that most men honored the sentiment.
</p>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/class.png"></a>
</div>
<p>There were nearly twice as many men as women who boarded the ship, but nearly twice as many women that survived than men.
</p>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/age.png"></a>
</div>
<p>And looking at age groups (children are under 18, elderly over 40 and adults between 18-40), it looks like you were much more likely to die in the frigid water if you were an adult male.<br>
</p>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/factorplot1.png"></a>
</div>
<p>On the other hand, your chances of survival look much better (relative to other men) if you were a high-roller in 1st class. Sorry Jack, you're dead. Ok this is the last reference to <i>Titanic.</i>
</p>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/giphy.gif"></a>
</div>
<p></p>
<p>One last point of note, which is surprising, is the difference in survival rates by port of embarkation. Southampton was by far the most popular port for departure, so it makes sense there are more people who died. However the chances of survival are lower than the other ports. It may be that more people that got on at Southampton that had third class cabins. The below regression chart also shows a dense clutter of people who would be grouped in the adult bin and we know that adults had the lowest survival rates. So a disproportionate number of adults in the third class may have contributed to Southampton's low survival rates. <br>
</p>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/port_regressions.png"></a>
</div>

<h2 class="section-heading">What Do the Models Say?</h2>
<p></p>
<p></p>

<p>Different classification models were trained and tested on split sample data. The standard 67% was used for model training and 33% for model testing. The features of each of the models are:
<ul>
  <li>sex</li>
  <li>normalized number of siblings and parents on the ship</li>
  <li><i>all normalization done with StandardScalar in sklearn's preprocessing module </i></li>
  <li>dummy variables:</li>
    <ul>
      <li>title (Mr., Mrs., etc.)</li>
        <ul>
          <li>Sidenote: Jonkheer was my favorite title, an old Dutch honorific which is technically a status and not a "title" ala Earl or Duke. It's the root word for Yonkers according to <a href="https://en.wikipedia.org/wiki/Jonkheer">Wikipedia</a>. Shout out to Yonkers! </li>
          <li>Sidenote part twee: That's twee as in two in Dutch. The Jonkheer who died on the Titanic was a one Johan George Reuchlin. He was a director of the Holland America Line and so got a free ticket on the world's largest, fastest, most luxious ship! And also a free ticket straight to his death. 
          </li>
        </ul>
      <li>port of embarkation</li>
      <li>passenger class</li>
      <li>age groups</li>
    </ul>

<p>For each model, the following metrics were calculated:
  <ul>
    <li>precision</li>
    <li>recall</li>
    <li>accuracy</li>
    <li>false positive rate</li>
    <li>area under ROC curve (AUC ROC)</li>
    <li>area under precision-recall curve (AUC PRC)</li>
    </ul>
</p>

<h3>Clarifying some terms</h3>
<p></p>
<p> If you're a data science wiz, go ahead and skip this section. If not, I'll do my best to convey what these terms mean as it relates to our data. The table below will help you visualize what I describe.
</p>

<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/fptable.png"></a>
</div>
Table source <a href= "https://social.msdn.microsoft.com/profile/Andreas+de+Ruiter+%28Microsoft%29">here</a>.

<p><b>Precision</b>: Measures how many passengers actually survived compared to how many were predicted to survive. <br>
Precision = True Positives / (True Positives + False Positives) <br>
</p>

<p><b>Recall</b>: Measures how many passengers were predicted to survive out of all the passengers that actually survived. <br>
Recall = True Positives / (True Positives + False Negatives)<br>
</p>

<p><b>False Positive Rate</b>: Simply put, it's the inverse of precision. <br>
FPR = False Positives / (True Positives + False Positives) <br>
</p>

<p><b>Accuracy</b>: Also called the F1 score, accuracy for our purposes is the balance between precision and recall. It's the harmonic mean between precision (positive predictive value) and recall (sensitivity) <br>
Accuracy = (Recall x Precision) / (Recall + Precision) x 2
</p>

<h3>Coefficients</h3>

<p></p>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/coeffs.png"></a>
</div>

<p> Just to make sense of our model, let's take a look at a few coefficents from the logistic regression.
<ul>
  <li><b>sex</b>: -1.50 makes sense as we set male to 1 and female to 0 and clearly the data shows that if you are a male, your chances of survival are pretty slim. This is also born out by <b>title_Mr</b> with a coefficient of -1.14. </li>

  <li><b> male other </b>: -1.25 shows these gents sacrified for the good of others. Passengers in this category (although few in number) were older men with more distinguished titles than Mr., such as Colonel, Captain or Don. Just guessing from their titles, I wonder if it was a greater sense of duty these men felt or they were just older and didn't make it to the lifeboats in time.</li>
  
  <li><b> port_S </b>: -0.11 coefficient may indicate, as mentioned above, that perhaps more adults with third class cabins boarded at Southampton.</li>
  
  <li><b> bin_child </b>: 0.90 is lower than I would like but maybe this is due more to children and babies being physically weaker (therefore unable to survive even if they made it on a lifeboat) than to adults taking their place on the lifeboats. </li>
</ul>
<p> All things considered, the coefficients are in line with my expectations.
</p>

<h2 class="section-heading">Back to the Models</h2>
<p></p>
<p></p>
<p> All of the models performed similarly with most of the model scores hovering around 0.80 with the exception of recall which ranged from 0.65-0.75. 
  <ul>
    <li>Logistic Regression</li>
    <li>Cross-validated Logistic Regression</li>
    <li>GridSearch Logistic Regression</li>
    <li>kNN Model</li>
    <li>GridSearch kNN Model</li>
    <li>GridSearch SVM Model</li>
    <li>GridSearch Decision Trees Model</li>
  </ul>
</p>

<h3> Cross-validated Logistic Regression Model </h3>
<p> 
</p>
<p>Built with scikitlearn's logisticregressionCV, this model performed the best all around with the highest accuracy, recall and ROC AUC scores. In general, I like the LogRegCV model for survivor prediction because I would rather have more false predictions for surviors and have people looking for them than to predict too few survivors and some people die waiting for help! 
</p>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/LRCV_cm.png"></a>
</div>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/LRCV_curves.png"></a>
</div>

<h3> GridSearch SVM Model</h3>
<p> 
</p>
<p>Built with optimized parameters on SVC (Support Vector Machines), this model had the lowest false positive rate (5.7%) and the highest precision (88.4%). The SVM model would be best if the client wanted a more conservative approach as this model favors predictive accuracy. However, this seems better for corporate purposes than rescue missions!
</p>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/SVM_cm.png"></a>
</div>
<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/SVM_curves.png"></a>
</div>


<h3> Model Metrics Table</h3>
<p> 
</p>

<div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-5/model_df.png"></a>
</div>

<h3> Final Thoughts</h3>
<p>
</p>
<p>All models were built using the train, test, split method. One drawback to this method is that it really limits the available test data as this database had less than 900 rows. The final test data sets contained approximately 300 rows, which may explain some of the angularity of the ROC and PRC curves. 
</p>
<p>Age bins (child, adult, elderly) and title were a good proxy for age and better for the model than actual age. About 20% of age values were missing and I filled them in by deriving a median group age value from passenger titles. These derived values may not be the most accurate reflection of real age distribution on the ship.
</p>
<p>Fare was eliminated because that information was better captured in passenger class (first, second, third). The fares varied widely even within the same passenger class, which may mean there were other pricing factors involved and it would've been noise to our model.
</p>
<p>Sibsp (number of siblings/spouse) was kept while parch (number of parents/children) was excluded because over several iterations of the logistic regression, parch did not have a significant effect (very small coefficients) to survival while sibsp had a justifiably large effect. It seems that in the parch parameter, being a parent who has a child (or children), and being a child who has parent(s), would cancel out each other's effect on survival probability in the model.
</p>
<p>Cabin would be an interesting parameter to look at but there were too many unique values of cabin (not passenger class as in first, second, third) to be a meaningful predictor in a small dataset. However cabins specify not just the class of a passenger but where on the ship the passenger is located. People closer to the lifeboats would have had a better chance of survival.
</p>  