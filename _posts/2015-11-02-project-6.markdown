---
layout:     post
title:      "Better Film Recommendations"
subtitle:   "Identifying Key Features of Highly Rated Films on IMDB"
date:       2016-11-02 12:00:00
author:     "Kristen Su"
header-img: "img/project-6/blackimdb.jpg"
---

<div>
<h2 class="section-heading">Executive Summary</h2>

  <p><b> Dataset: <a href="http://www.imdb.com/chart/top" target="_blank"> IMDB Top 250 Films of All Time </a> </b></p>
  <p> The goal of the project was to examine what factors contribute to movie ratings. I used tree-based machine learning models to identify movie feature importance in determining user ratings for the top 250 films as rated by IMDB users. 
  </p>

  <p><b>Project Details</b>:
  <ol>
    <li> Use the OMDB API to get information on top-rated films on IMDB </li>
    <li> Webscrape additional information directly from IMDB (gross revenue, opening weekend revenue, film budget) </li>
    <li> Use CountVectorizer to perform Natural Language Processing on key words (plot, title, actors, directors, etc) </li>
    <li> Create regression and classification models (decision trees and random forests) to determine most important movie features in predicting user ratings </li>
    <li>Cross validate model scores</li>
    <li>Models used:</li>
      <ul>
        <li>DecisionTree</li>
        <li>RandomForest</li>
        <li>ExtraTrees</li>
        <li>AdaBoost</li>
        <li>GradientBoosting</li>
      </ul>
      <li>Bagging and GridSearch were also used on certain models where appropriate </li>
  </ol>


<h2 class="section-heading">The Paradox of Choice</h2>
<br>

    <div align = 'center'>  
        <a href="#">
          <img src="{{ site.baseurl }}/img/project-6/streaming2.jpg"></a>
    </div>
    <br>

  <p> We have nearly endless choice in movies with Netflix, Amazon Prime, Hulu, HBO Now and many more on demand streaming services. But have you ever spent an entire night just browsing through the options and unable to land on anything? And the site algorithm keeps giving you terrible recommendations?
  <br>
  <br>
  This is such a common phenomenon that even The Onion riffed on it with their segment <a href="http://www.theonion.com/video/netflix-introduces-new-browse-endlessly-plan-35308" target="_blank">"Netflix Introduces New 'Browse Endlessly' Plan"</a>. The report says, "the inexpensive new plan lets users just browse all the titles and posters they want for just five dollars a month." For more on the downsides of too much choice, check out Barry Schwartz's <a href="https://www.ted.com/talks/barry_schwartz_on_the_paradox_of_choice" target="_blank">TED Talk</a>. 
  <br>
  <br>
  Now I'm not advocating a less is more approach because we are way past that point. But it never hurts better to try and create a better algorithm. One that curates content better and matches a user's idiosyncratic tastes. One that essentially limits the choices down to the very best ones. Just because you love Tarantino's films doesn't mean you will like all kinds of blood-soaked violence intermixed with incessent yet indelible dialogue. 
  </p>

    <div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-6/recommendations.jpg"></a>
    </div>

  <p>But how can an algorithm recognize the gems without also suggesting irrelevent duds? One way to approach this problem would be to look at the most highly-rated films and investigate what about these films make them so popular and well-regarded. 
  </p>


<h2 class="section-heading">Webscraping</h2>
<br>

  <p>Thank you Beautiful Soup for making webscraping in python so efficient! Here's a snippet of code that takes a list of IMDB film IDs and returns information on those films through an api call on omdbapi.com. 
  </p>

<div align = 'center'>  
      <a href="#">
        <img src="{{ site.baseurl }}/img/project-6/omdbapi.png"></a>
    </div>
    
  <p>I took the output from the api call and scraped the gross revenue, opening weekend revenue and budget information directly from IMDB to get my final dataset. 
  <br>
  Unfortunately more than half of the opening weekend numbers were missing so I couldn't use it as a feature. Also 38 budget values were missing and with no easy way to impute that data, I decided to exclude budget as well. 
  </p>

<h2 class="section-heading">Missing Values, Always Fun! </h2>
<br>
<h3>Metascore</h3>

<p>
  <b>Quick definition of various film scoring metrics:</b>
  <ul>
    <li><b>Metascore</b>: Score based on weighted ratings from professional film critics (weighting based on a critic's fame); from metacritic.com </li>
    <li><b>tomatoMeter</b>: Score that represents the percentage of positive professional critic reviews; from rottentomatoes.com </li>
    <li><b>tomatoRating</b>:  Score derived from tomatoMeter. </li>
  </ul>
</p>

<p>There were 84 Metascore values missing out of 250 total samples, about 33% of the data. I believed that Metascore was an important metric for film ratings but obviously not useful with so much of the data missing. 
</p>

<p>Luckily when I checked the correlation of Metascore against two similar film scoring metrics, tomatoMeter and tomatoRating, the correlation was high enough that I could use the latter two scores in place of Metascore. There were several other film rating metrics available in the dataset, but tomatoMeter and tomatoRating were the most highly correlated and therefore the best proxies.
</p>

  <div align = 'center'>  
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-6/score_df_heatmap.png"></a>
  </div>
<br>
  <div>
    <a href="#">
      <img src="{{ site.baseurl }}/img/project-6/metascore_heatmap2.png"></a>
  </div>

<h3>Gross Revenue</h3>

<p>I also believed that the gross revenue of a film would be a great predictor of film ratings. The thought process was that the more money a film made, the more people went to see it and thus the more popular it was. The more popular a film, the more likely that it would be rated in the first place and it would be rated highly.
</p>

<p>There were 38 missing revenue values in the dataset and I decided to run a linear regression using the subset of data with film revenue to predict and fill in the missing revenue. Unfortunately, the model score of 0.611 was too low to use for predicting missing revenue numbers.  
</p>

<p>On the bright side, it turns out that gross revenue is not very highly correlated with rating at all! Of course I hadn't adjusted for inflation and as the oldest film in the dataset was from 1921 (The Kid) and the newest from 2016 (Zootopia, Deadpool), this may severely skew the data. 
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/gross_regplots2.png"></a>
</div>

<p>However, even if I had adjusted for inflation there would still be some incongruence in comparability as the distribution and availability of movies is much greater now than ever. A more apples to apples comparison may be to just examine top rated films from the past 10-20 years. 
</p>

<p>The most interesting observations on the data come from plotting the release year for a film vs. other features. The more recent a film, the more imdb votes it received and the more varied the ratings. The older a film, the fewer votes it received but the higher the ratings. 
</p>

<p><b>What does this tell us? </b></p>
<p>Just because a film is highly rated (high ratings) does not mean it is necessarily popular (low number of ratings). Already we see a vulnerability in creating a better recommendation algorithm just by examining the top user-rated films.
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/year_regplots2.png"></a>
</div>

<p><b>How do we get to gross revenue?</b></p>
<p>We still need a way to derive gross revenue. I didn't want to just fill in all missing data with the median value. Instead I created the correlation index, the correlation (0.54) between gross revenue and tomatoFresh score, as these this was the most highly correlated with gross revenue. Then I applied this index to actual gross revenue for data with a revenue value and to the median for data with revenue missing. This way the revenue numbers would be tempered by the correlation index and maybe less distorted than if I had just used median to fill in missing values.
</p>
<blockquote> Adjusted Gross = Gross Revenue * Correlation Index </blockquote>
<blockquote> Adjusted Gross = Median Revenue * Correlation Index </blockquote>

<h2 class="section-heading">Natural Language Processing</h2>
<br>
<p>I extracted key words from language-related features using sklearn's CountVectorizer. I wanted to see which actors, directors, plot and title key-words, genres and production companies most affect ratings. Are  Hollywood A-listers worth the huge cost of their contracts? Are certain words in the title more inclined to higher ratings than others? 
</p>

<p>I limited the key words to the top 25 in each category as I did not want too many features to flow into a model with a very small number of samples. In other words, I didn't want to crazy overfit my models. 
</p>

<h3>Actors</h3>
<p><b>Key takeaway</b>:
<br>
Have men in your film.

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/topactors.png"></a>
</div>
<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/topactors_first.png"></a>
</div>

<h3>Plot Key Words</h3>
<p><b>Key takeaway</b>:
<br>
War or police-related movies involving a young father in a new world looks like a winning recipe.
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/topplot.png"></a>
</div>

<h3>Title Key Words</h3>
<p><b>Key takeaway</b>:
<br>
You can really see the data distortion from having such a small sample size.
<br>
Star Wars = top rating.
<br>
Lord of the Rings = top rating.
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/toptitle.png"></a>
</div>

<h2 class="section-heading">Tree-Based Modeling</h2>
<br>

<p>I trained and tested tree-based models to determine feature importance for user ratings. Regressor and classifier models were both used. The float value of imdb Ratings (from 0-10) were used as the target variable for regressors and three classification bins (average, good, excellent) were used for classifiers.
</p> 
<p>Model performance was evaluated by examinging cross-val scores; mean-squared error for regressors and accuracy scores for classifiers.
</p>

<p> <b>Target: imdb Rating</b>
</p>
<p> <b>Features: </b> 
<ul>
  <li>Adjusted gross revenue</li>
  <li>Professional film ratings: tomatoMeter, tomatoRating, etc</li>
  <li>User film ratings: tomatoUserMeter, tomatoUserRating, etc</li>
  <li>Awards: Oscar wins and nominations; other award wins and noms</li>
  <li>Other film features: runtime, rating, release date </li>
  <li>NLP features: actors, directors, plot, title, genre, production company</li>
</ul>
</p>

<p>Note: Although the number of votes cast on imdb was the most highly correlated to imdbRating, I decided not to include this feature in the models for two reasons:
<ol>
  <li>Number of votes cast is not an inherent feature of a film</li>
  <li>A recommendation algorithm on any given platform would not initially have number of ratings available as a feature</li>
</ol>
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/imdb_corr.png"></a>
</div>

<h3>Best Performing Regressor Models</h3>
<p> GridSearchCV DecisionTree Regressor performed the best overall, with a cross-validated MSE score of 1.0 followed by GridSearchCV RandomForest Regressor with a 0.78 MSE. 
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/topreg.png"></a>
</div>

<p> One reason for such high scores, even when cross-validated, may be that the range of imdb Ratings for the test data was very narrow - 8.0 and 10.0.
</p>

<p>Also the regressors take Rotten Tomatoes data, especially user-generated ratings, as the most important features as illustrated below. It makes sense that user ratings of a film on one platform would be consistent to ratings on another platform given the wide-appeal of both the imdb and Rotten Tomatoes websites and therefore the intermixing of users.
</p>

<p>It's also worth mentioning that film scoring metrics generated by professional critics' reviews did not make it into the top features. Perhaps critical acclaim doesn't carry the cache one might expect.
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/topregfeatures.png"></a>
</div>

<h3>Best Performing Classifier Models</h3>
<p> GridSearchCV GradientBoosting Classifier performed the best with an accuracy score of 0.63 followed closely by both the standard GradientBoosting Classifier and DecisionTree Classifier with accuracy scores of 0.604 and 0.600 respectively.
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/topclass.png"></a>
</div>

<p> It seems that the classifier models were not as clear cut as the regressors. When looking at feature importance, there is a greater range of features used in making predictions and the highest feature importance across all classifiers was about 22% vs. about 45% for some regressor models.
</p>

<p> The classifier models also have user-generated ratings and reviews as key features, but also includes Year, Adj. Gross and Runtime. This is more in line with what I would expect, especially from a more generalized dataset.
</p>

<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/topclassfeatures.png"></a>
</div>

<p> Below is the confusion matrix for the best performing classifier, the GridSearchCV GradientBoosting Classifier.
</p>
<div>
  <a href="#">
    <img src="{{ site.baseurl }}/img/project-6/confusionmatrix_df.png"></a>
</div>

<h2 class="section-heading">Conclusion</h2>
<br>
<p> Both the regressor and classifier models did well, with the regressor tree-based models having incredibly high scores. However we used a very small dataset of only 250 samples and these samples were highly skewed toward already popular and highly-rated films. Taking this into consideration, the classifier models should have done better!
</p>
<p> The logical next step would be to go back and scrape a more comprehensive list of films so that our models can generalize better and maybe limit the release year to the last 20 years. The nature of the movie business has evolved too much over the past 90 years to have a clear apples to apples comparison of films across eras. Also we could get a better comparison of gross revenue, which I still believe can play an important role in recommendation algorithms.
</p>
<p> Finally I wonder if we could substitute Rotten Tomatoes user-generated ratings, which are very much in line with imdb ratings, with other features. I don't want to use ratings from one platform to predict ratings on another platform. I want to use inherent features in a movie. Perhaps we can add special effects and other technical aspects of a movie, more about film distribution (domestic, interational, number of theaters) and more detailed breakdown of current features. For example actors can be further broken down by lead or supporting actors. Stay tuned, I may decide to further tune my modeling and analysis!
</p>
